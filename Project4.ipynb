{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d54daeb-b4e5-4357-9db5-d3fd94648cab",
   "metadata": {},
   "source": [
    "# project 4\n",
    "---\n",
    "## Predicting The 2024 Presidential Election outcome with Machine Learning algorithm\n",
    "---\n",
    "### Retrieving the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f7b27d-545a-4950-80f2-97c4040c07d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import findspark and initialize. \n",
    "# import findspark\n",
    "# findspark.init()\n",
    "\n",
    "# Import packages\n",
    "# from pyspark.sql import SparkSession\n",
    "# import time\n",
    "\n",
    "# Create a SparkSession\n",
    "#spark = SparkSession.builder.appName(\"SparkSQL\").getOrCreate()\n",
    "# 1. Read in the AWS S3 bucket into a DataFrame.\n",
    "#from pyspark import SparkFiles\n",
    "#url = \"https://2u-data-curriculum-team.s3.amazonaws.com/dataviz-classroom/v1.2/22-big-data/home_sales_revised.csv\"\n",
    "#spark.sparkContext.addFile(url)\n",
    "# df = spark.read.csv(SparkFiles.get(\"home_sales_revised.csv\"), sep=\",\", header=True)\n",
    "# df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f30243c1-27c6-4de3-ba51-ebdaf0fb30d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our dependencies\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "\n",
    "#  Import and read the dataset\n",
    "df = pd.read_csv(\" \")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c87fc6-9944-4c11-8698-e06c32eff2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot your data to see what's in your DataFrame\n",
    "df_market_data.hvplot.line(\n",
    "    width=800,\n",
    "    height=400,\n",
    "    rot=90\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4185025-ac59-47ff-b341-30b4e38e881c",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4714d674-acae-474c-bc3c-a8e752ae0ce8",
   "metadata": {},
   "source": [
    "#### Data cleaning and normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f97a71-34b0-4530-9084-d210c1099652",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb97560-401e-4e38-b1b3-ad4bd695df4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db698e9c-bab9-4454-a47f-f15b153a520d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b024c1e9-29e1-4864-97f9-3ae20d029b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the number of unique values in each column.\n",
    "df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc694957-0300-458d-a083-5944112a5c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the non-beneficial columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee64d06-d21e-44f4-b1ab-22c608e561c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical data to numeric with `pd.get_dummies`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5af60f-8b44-40ff-98e1-7af6580c5fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split our preprocessed data into our features and target arrays\n",
    "y = \n",
    "X = \n",
    "\n",
    "# Split the preprocessed data into a training and testing dataset\n",
    "X_train , X_test , y_train , y_test = train_test_split(X,y , random_state = 78)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15d23dc-107f-405b-bfab-7a75f58fbe20",
   "metadata": {},
   "source": [
    "## Create a Logistic Regression Model with the Original Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d3ec15-7249-492e-92b1-229b83657ca6",
   "metadata": {},
   "source": [
    "###  Step 1: Fit a logistic regression model by using the training data (`X_train` and `y_train`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541325d2-57ed-4518-97ee-0f50f3981606",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import the LogisticRegression module from SKLearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Instantiate the Logistic Regression model\n",
    "# Assign a random_state parameter of 1 to the model\n",
    "classifier = LogisticRegression(solver = 'lbfgs' , random_state = 1)\n",
    "classifier\n",
    "\n",
    "# Fit the model using training data\n",
    "classifier.fit(X_train , y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cee6b5-2e7a-49ec-ab5e-7ffa98977127",
   "metadata": {},
   "source": [
    "### Step 2: Save the predictions on the testing data labels by using the testing feature data (`X_test`) and the fitted model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b54b4d-b0ab-436e-b05c-d3f848436230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a prediction using the testing data\n",
    "predictions = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9714ce47-8fd9-43ab-a6d3-f8f535d4a4b1",
   "metadata": {},
   "source": [
    "### Step 3: Evaluate the modelâ€™s performance by doing the following:\n",
    "\n",
    "* Generate a confusion matrix.\n",
    "* Print the classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89498cb9-a5ea-4170-866d-3ffd287d9067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a confusion matrix for the model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b7e09c-5951-4cb5-9eee-5c79177a8eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the classification report for the model\n",
    "from sklearn.metrics import classification_report\n",
    "target_names = [\"healthy loan\", \"high-risk loan)\"]\n",
    "print(classification_report(y_test, predictions, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3331377-3e8b-438f-953a-6e844cbddac9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3159fa-dbbb-40b0-9aef-241f07d5bac1",
   "metadata": {},
   "source": [
    "## Create a Neural Network Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84be46d7-0976-47ef-9650-94f7c9c06e02",
   "metadata": {},
   "source": [
    "### Step 1: Initialize a StandardScaler instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0c93b1-607e-40c3-8e09-4bcb40eecaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StandardScaler instances\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the StandardScaler\n",
    "X_scaler = scaler.fit(X_train)\n",
    "\n",
    "# Scale the data\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba966f2-eb59-4ec8-ba4a-5fffc25845bb",
   "metadata": {},
   "source": [
    "### Step 2: Compile, Train and Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110671be-6636-46c9-9c3b-7da213ee90c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model - deep neural net, i.e., the number of input features and hidden nodes for each layer.\n",
    "number_input_features = len(X_train[0])\n",
    "hidden_nodes_layer1 = 8\n",
    "hidden_nodes_layer2 = 5\n",
    "\n",
    "\n",
    "nm = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nm.add(tf.keras.layers.Dense(units = hidden_nodes_layer1, activation=\"relu\", input_dim = number_input_features))\n",
    "\n",
    "# Second hidden layer\n",
    "nm.add(tf.keras.layers.Dense(units = hidden_nodes_layer2 , activation = \"sigmoid\" ))\n",
    "\n",
    "# Output layer\n",
    "nm.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "# Check the structure of the model\n",
    "nm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a80c68-7906-4888-b6d4-1b63bae2f208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "nm.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3dcbca-74fc-493e-b01f-1225f3cb3633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "fit_model = nm.fit(X_train_scaled , y_train , epochs = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d152f9-5575-4a4a-8378-1fb52c5d53d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model using the test data\n",
    "model_loss, model_accuracy = nm.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52758051-6a06-4071-aec2-778b2cdbddc1",
   "metadata": {},
   "source": [
    "### Step 3: Optimize the model with the keras_tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0413be-7fc5-4339-a58b-97eb96553475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a method that creates a new Sequential model with hyperparameter options\n",
    "def create_model(hp):\n",
    "    nm_model = tf.keras.models.Sequential()\n",
    "\n",
    "    # Allow kerastuner to decide which activation function to use in hidden layers\n",
    "    activation = hp.Choice('activation',['relu','tanh','sigmoid'])\n",
    "\n",
    "    # Allow kerastuner to decide number of neurons in first layer\n",
    "    nm_model.add(tf.keras.layers.Dense(units=hp.Int('first_units',\n",
    "        min_value=1,\n",
    "        max_value=10,\n",
    "        step=2), activation=activation, input_dim=43))\n",
    "\n",
    "    # Allow kerastuner to decide number of hidden layers and neurons in hidden layers\n",
    "    for i in range(hp.Int('num_layers', 1, 6)):\n",
    "        nm_model.add(tf.keras.layers.Dense(units=hp.Int('units_' + str(i),\n",
    "            min_value=1,\n",
    "            max_value=10,\n",
    "            step=2),\n",
    "            activation=activation))\n",
    "\n",
    "    nm_model.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "    # Compile the model\n",
    "    nm_model.compile(loss=\"binary_crossentropy\", optimizer='adam', metrics=[\"accuracy\"])\n",
    "\n",
    "    return nm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd9dc6c-3a61-4233-a042-096b7a1083a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the kerastuner library\n",
    "import keras_tuner as kt\n",
    "\n",
    "tuner = kt.Hyperband(\n",
    "    create_model,\n",
    "    objective=\"val_accuracy\",\n",
    "    max_epochs=20,\n",
    "    hyperband_iterations=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f0bd9e-28be-4bfc-b34b-0dc9f821d58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the kerastuner search for best hyperparameters\n",
    "tuner.search(X_train_scaled,y_train,epochs=20,validation_data=(X_test_scaled,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793e10f9-5a85-419f-a552-fb123d6aa8da",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Get best model hyperparameters\n",
    "best_hyper = tuner.get_best_hyperparameters(1)[0]\n",
    "best_hyper.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6cb9a3-69b1-4abe-9fdd-2226730a1738",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = tuner.get_best_models(1)[0]\n",
    "model_loss, model_accuracy = best_model.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc70144-ee20-442d-a36f-cd24a0a0a8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export our model to HDF5 file\n",
    "best_model.save(\"Project4.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef2d570-3433-4bb0-8636-b2aa06c1382c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c42a61-f8f9-4adc-86a8-f35aa084c864",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
